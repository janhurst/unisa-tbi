{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bitcapstoneconda95c7eaee4aa645f6a7eec5cde52f040b",
   "display_name": "Python 3.8.1 64-bit ('capstone': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "START: 00-load-raw-data.ipynb\n  PECARN TBI data read from c:\\Jan\\Capstone\\data/TBI PUD 10-08-2013.csv into \"pecarn_tbi\" dataframe\nSTART: 01-data-cleaning.ipynb\n  Dropping records where GCS < 14\n  Dropping GCS columns as they are now redundant\n  Dropping AgeInMonth\n  Renaming AgeinYears to Age\n  Dropping EmplType\n  Dropping AgeInMonth\n  Dropping High_impact_InjSev\n  Renaming InjuryMech to Injury_Mechanism\n  Renaming ActNorm to Acting_Normal\n  Setting Acting_Normal missing data to 1 (Yes)\n  Dropping Findings## columns\n  The cleaned dataset is now available in a dataframe named \"data\"\nSTART: 02-data-imputation.ipynb\n  Temporary Fix - dropping Age\n  Removing 54459 NaN values from 41 columns\n  The imputed dataset is now available in a dataframe named \"data_imputed\"\n"
    }
   ],
   "source": [
    "# call the 01-data-cleaning.ipynb notebook to bring the pecarn_tbi dataframe and the cleaned dataframe into the environment\n",
    "%cd -q ../notebooks\n",
    "%run ./02-data-imputation.ipynb\n",
    "%cd -q -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "We know the data is categorical, so we can One Hot Encode ahead of time.\n",
    "Note - this means we should configure a custom TPOT pipeline and remove some of the preprocessing options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# don't one-hot-encode the class variable\n",
    "data_inputs = data_imputed.drop(columns='PosIntFinal')\n",
    "\n",
    "# encode\n",
    "encoder = preprocessing.OneHotEncoder(sparse=False, dtype=np.int)\n",
    "encoder.fit(data_inputs)\n",
    "data_encoded = encoder.transform(data_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrange Data\n",
    "Need to convert the dataframes to numpy arrays so that TPOT can work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Y = data_imputed['PosIntFinal'].astype('int64').to_numpy()\n",
    "data_X = data_encoded.astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure TPOT\n",
    "Initial attempts to use the TPOT Light template were taking a long time too complete.\n",
    "\n",
    "We create a tpot_config that is based on the TPOT Light template, and removing in particular preprocessors that we aren't interested in.\n",
    "\n",
    "See https://github.com/EpistasisLab/tpot/blob/master/tpot/config/classifier_light.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/EpistasisLab/tpot/blob/master/tpot/config/classifier_light.py\n",
    "tpot_config = {\n",
    "\n",
    "    # Classifiers\n",
    "    'sklearn.naive_bayes.GaussianNB': {\n",
    "    },\n",
    "\n",
    "    'sklearn.naive_bayes.BernoulliNB': {\n",
    "        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],\n",
    "        'fit_prior': [True, False]\n",
    "    },\n",
    "\n",
    "    'sklearn.naive_bayes.MultinomialNB': {\n",
    "        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],\n",
    "        'fit_prior': [True, False]\n",
    "    },\n",
    "\n",
    "    'sklearn.tree.DecisionTreeClassifier': {\n",
    "        'criterion': [\"gini\", \"entropy\"],\n",
    "        'max_depth': range(1, 11),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21)\n",
    "    },\n",
    "\n",
    "\n",
    "    'sklearn.neighbors.KNeighborsClassifier': {\n",
    "        'n_neighbors': range(1, 101),\n",
    "        'weights': [\"uniform\", \"distance\"],\n",
    "        'p': [1, 2]\n",
    "    },\n",
    "\n",
    "\n",
    "    'sklearn.linear_model.LogisticRegression': {\n",
    "        'penalty': [\"l1\", \"l2\"],\n",
    "        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],\n",
    "        'dual': [True, False]\n",
    "    },\n",
    "\n",
    "    # Preprocesssors\n",
    "    # 'sklearn.preprocessing.Binarizer': {\n",
    "    #     'threshold': np.arange(0.0, 1.01, 0.05)\n",
    "    # },\n",
    "\n",
    "    'sklearn.cluster.FeatureAgglomeration': {\n",
    "        'linkage': ['ward', 'complete', 'average'],\n",
    "        'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine']\n",
    "    },\n",
    "\n",
    "    # 'sklearn.preprocessing.MaxAbsScaler': {\n",
    "    # },\n",
    "\n",
    "    # 'sklearn.preprocessing.MinMaxScaler': {\n",
    "    # },\n",
    "\n",
    "    # 'sklearn.preprocessing.Normalizer': {\n",
    "    #     'norm': ['l1', 'l2', 'max']\n",
    "    # },\n",
    "\n",
    "    'sklearn.decomposition.PCA': {\n",
    "        'svd_solver': ['randomized'],\n",
    "        'iterated_power': range(1, 11)\n",
    "    },\n",
    "\n",
    "    # 'sklearn.kernel_approximation.RBFSampler': {\n",
    "    #     'gamma': np.arange(0.0, 1.01, 0.05)\n",
    "    # },\n",
    "\n",
    "    # 'sklearn.preprocessing.RobustScaler': {\n",
    "    # },\n",
    "\n",
    "    # 'sklearn.preprocessing.StandardScaler': {\n",
    "    # },\n",
    "\n",
    "    'tpot.builtins.ZeroCount': {\n",
    "    },\n",
    "\n",
    "    # Selectors\n",
    "    'sklearn.feature_selection.SelectFwe': {\n",
    "        'alpha': np.arange(0, 0.05, 0.001),\n",
    "        'score_func': {\n",
    "            'sklearn.feature_selection.f_classif': None\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.SelectPercentile': {\n",
    "        'percentile': range(1, 100),\n",
    "        'score_func': {\n",
    "            'sklearn.feature_selection.f_classif': None\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.VarianceThreshold': {\n",
    "        'threshold': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_X, data_Y, train_size=0.75, test_size=0.25)\n",
    "\n",
    "tpot = TPOTClassifier(config_dict=tpot_config,\n",
    "                      generations=3,\n",
    "                      population_size=10, \n",
    "                      memory='auto',\n",
    "                      n_jobs=-1,\n",
    "                      #early_stop=5,\n",
    "                      verbosity=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Optimization Progress:   0%|          | 0/40 [00:00<?, ?pipeline/s]12 operators have been imported by TPOT.\nOptimization Progress:  25%|██▌       | 10/40 [05:42<2:51:17, 342.60s/pipeline]Skipped pipeline #3 due to time out. Continuing to the next pipeline.\nSkipped pipeline #7 due to time out. Continuing to the next pipeline.\nSkipped pipeline #10 due to time out. Continuing to the next pipeline.\nOptimization Progress:  68%|██████▊   | 27/40 [11:31<53:42, 247.87s/pipeline]Skipped pipeline #14 due to time out. Continuing to the next pipeline.\nSkipped pipeline #21 due to time out. Continuing to the next pipeline.\nSkipped pipeline #24 due to time out. Continuing to the next pipeline.\nSkipped pipeline #26 due to time out. Continuing to the next pipeline.\nGeneration 1 - Current Pareto front scores:\n-1\t0.9999057295480321\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=8, DecisionTreeClassifier__min_samples_leaf=20, DecisionTreeClassifier__min_samples_split=6)\n\nPipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\nOptimization Progress:  92%|█████████▎| 37/40 [11:40<08:41, 173.69s/pipeline]Generation 2 - Current Pareto front scores:\n-1\t0.9999057295480321\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=8, DecisionTreeClassifier__min_samples_leaf=20, DecisionTreeClassifier__min_samples_split=6)\n\n_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\nOptimization Progress: 47pipeline [16:17, 130.83s/pipeline]Generation 3 - Current Pareto front scores:\n-1\t0.9999057295480321\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=8, DecisionTreeClassifier__min_samples_leaf=20, DecisionTreeClassifier__min_samples_split=6)\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TPOTClassifier(config_dict={'sklearn.cluster.FeatureAgglomeration': {'affinity': ['euclidean',\n                                                                                  'l1',\n                                                                                  'l2',\n                                                                                  'manhattan',\n                                                                                  'cosine'],\n                                                                     'linkage': ['ward',\n                                                                                 'complete',\n                                                                                 'average']},\n                            'sklearn.decomposition.PCA': {'iterated_power': range(1, 11),\n                                                          'svd_solver': ['randomized']},\n                            'sklearn.feature_selection.SelectFwe': {'alpha': array([0.   , 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007...\n                            'tpot.builtins.ZeroCount': {}},\n               crossover_rate=0.1, cv=5, disable_update_check=False,\n               early_stop=None, generations=3, max_eval_time_mins=5,\n               max_time_mins=None, memory='auto', mutation_rate=0.9, n_jobs=-1,\n               offspring_size=None, periodic_checkpoint_folder=None,\n               population_size=10, random_state=None, scoring=None,\n               subsample=1.0, template=None, use_dask=False, verbosity=3,\n               warm_start=False)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "tpot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9999057315233786\n"
    }
   ],
   "source": [
    "print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.export('tpot_tbi_pipeline.py')"
   ]
  }
 ]
}